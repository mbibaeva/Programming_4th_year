{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.web import Wikipedia, plaintext\n",
    "#from string import punctuation\n",
    "\n",
    "import re\n",
    "from string import digits, punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам необходимо написать программу, которая парсит статьи википедии (language=en) и считает по ним некоторые статистики.\n",
    "\n",
    "1. Реализовать класс WikiParser с конструктором без аргументов и методом get_articles, который принимает название исходной статьи start, обходит все статьи википедии, на которые она ссылается, и возвращает список содержимого статей (с выполнением парсинга).\n",
    "\n",
    "    При парсинге каждой статьи для того, чтобы убрать html теги, используется функция pattern.web.plaintext.\n",
    "    В результате парсинга между соседними словами должно быть 1 пробел.\n",
    "    В результате парсинга весь текст переводится в lowercase, знаки пунктуации выкидываются\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    " # task 1\n",
    "class WikiParser:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', digits))\n",
    "        text = text.translate(str.maketrans('', '', punctuation)) #не удаляет дефисы, но думаю, это уместно\n",
    "        clean_text = re.sub('\\s{2,}', ' ', text)\n",
    "        return clean_text\n",
    "    \n",
    "    def get_articles(self, start):\n",
    "        wiki = Wikipedia(language='en')\n",
    "        start = wiki.article(start)\n",
    "        list_of_strings = list()\n",
    "        for link in start.links:\n",
    "            article = wiki.article(link)\n",
    "            if article is not None:\n",
    "                article = plaintext(article.source)\n",
    "                list_of_strings.append(self.clean_text(article))\n",
    "        return list_of_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать класс TextStatistics с конструктором, который принимает в качестве аргумента список содержимого статей:\n",
    "\n",
    "    get_top_3grams - возвращает tuple, первым элемент которого - список 3-грамм в порядке убывания их частот, второй элемент - соотвественно список сколько раз встретилась каждая 3грамма. Подсчет идет по всему корпусу articles. При подсчете 3-грамм исключить из рассмотрения все числа и пунктуацию.\n",
    "    get_top_words - возвращает tuple, первым элемент которого - список слов в порядке убывания их частот, второй элемент - соотвественно список сколько раз встретилась каждое слово. Подсчет идет по всему корпусу articles. При подсчете слов исключить из рассмотрения все числа, предлоги, артикли и пунктуацию.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # task 2\n",
    "class TextStatistics:\n",
    "    def __init___(self):\n",
    "        pass\n",
    "    \n",
    "    def get_top_3grams(self, articles, n):\n",
    "        all_3grams = list()\n",
    "        for article in articles:\n",
    "            for ngram in ngrams(word_tokenize(article), 3):\n",
    "                all_3grams.append(ngram)\n",
    "        all_3grams = Counter(all_3grams)\n",
    "        ngram_list = list()\n",
    "        freq_list = list()\n",
    "        for ngram, freq in all_3grams.most_common()[:n]:\n",
    "            ngram_list.append(ngram)\n",
    "            freq_list.append(freq)\n",
    "        return (ngram_list, freq_list)\n",
    "    \n",
    "    def get_top_words(self, articles, n):\n",
    "        words = list()\n",
    "        for article in articles:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            tokens = tokenizer.tokenize(article)\n",
    "            for token in tokens:\n",
    "                if token not in stop_words:\n",
    "                    words.append(token)\n",
    "        words = Counter(words)\n",
    "        top_words = words.most_common()[:n]\n",
    "        word_list = list()\n",
    "        freq_list = list()\n",
    "        for word, freq in top_words:\n",
    "            word_list.append(word)\n",
    "            freq_list.append(freq)\n",
    "        return (word_list, freq_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать класс Experiment с методом show_results, который используя WikiParser и TextStatistics:\n",
    "\n",
    "    Выполняет парсинг статей википедии для \"Natural language processing\"\n",
    "    По полученному корпусу текстов считает топ-20 3-грамм и топ-20 слов.\n",
    "    По статье \"Natural language processing\" (только по ней) считает топ-5 3-грамм и топ-5 слов.\n",
    "    Печатает результаты эксперимента в структурированной форме\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 3\n",
    "class Experiment:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "            \n",
    "    def show_results(self):\n",
    "        corpus = WikiParser().get_articles('Natural language processing')\n",
    "        top_words = TextStatistics().get_top_words(corpus, 20)\n",
    "        print('Top-20 words of the whole corpora' + '\\n')\n",
    "        for x, y in zip(top_words[0],top_words[1]):\n",
    "            print(x + ' - ' + str(y) + '\\n')\n",
    "            \n",
    "        top_3grams = TextStatistics().get_top_3grams(corpus, 20)\n",
    "        print('\\n' + 'Top-20 3grams of the whole corpora' + '\\n')\n",
    "        for x, y in zip(top_3grams[0],top_3grams[1]):\n",
    "            print(str(x) + ' - ' + str(y) + '\\n')\n",
    "        \n",
    "        nlp = plaintext(wiki.article('Natural language processing').source)\n",
    "        nlp = [WikiParser().clean_text(nlp)]\n",
    "        \n",
    "        top_5_words = TextStatistics().get_top_words(nlp, 5)\n",
    "        print('\\n' + 'Top-5 words of \\'Natural language processing\\'' + '\\n')\n",
    "        for x, y in zip(top_5_words[0],top_5_words[1]):\n",
    "            print(x + ' - ' + str(y) + '\\n')\n",
    "            \n",
    "        top_5_3grams = TextStatistics().get_top_3grams(nlp, 5)\n",
    "        print('\\n' + 'Top-5 3grams of \\'Natural language processing\\'' + '\\n')\n",
    "        for x, y in zip(top_5_3grams[0],top_5_3grams[1]):\n",
    "            print(str(x) + ' - ' + str(y) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-20 words of the whole corpora\n",
      "\n",
      "language - 3970\n",
      "\n",
      "english - 1836\n",
      "\n",
      "speech - 1754\n",
      "\n",
      "retrieved - 1697\n",
      "\n",
      "languages - 1687\n",
      "\n",
      "words - 1649\n",
      "\n",
      "also - 1643\n",
      "\n",
      "word - 1565\n",
      "\n",
      "used - 1534\n",
      "\n",
      "text - 1487\n",
      "\n",
      "p - 1411\n",
      "\n",
      "may - 1309\n",
      "\n",
      "displaystyle - 1241\n",
      "\n",
      "one - 1240\n",
      "\n",
      "machine - 1227\n",
      "\n",
      "learning - 1188\n",
      "\n",
      "example - 1129\n",
      "\n",
      "isbn - 1122\n",
      "\n",
      "use - 1068\n",
      "\n",
      "b - 1041\n",
      "\n",
      "\n",
      "Top-20 3grams of the whole corpora\n",
      "\n",
      "('from', 'the', 'original') - 378\n",
      "\n",
      "('archived', 'from', 'the') - 366\n",
      "\n",
      "('natural', 'language', 'processing') - 329\n",
      "\n",
      "('the', 'original', 'on') - 308\n",
      "\n",
      "('v', 't', 'e') - 251\n",
      "\n",
      "('the', 'use', 'of') - 228\n",
      "\n",
      "('as', 'well', 'as') - 210\n",
      "\n",
      "('one', 'of', 'the') - 210\n",
      "\n",
      "('proceedings', 'of', 'the') - 187\n",
      "\n",
      "('a', 'b', 'c') - 183\n",
      "\n",
      "('the', 'european', 'union') - 164\n",
      "\n",
      "('cambridge', 'university', 'press') - 153\n",
      "\n",
      "('such', 'as', 'the') - 151\n",
      "\n",
      "('of', 'the', 'european') - 151\n",
      "\n",
      "('the', 'number', 'of') - 142\n",
      "\n",
      "('for', 'example', 'the') - 137\n",
      "\n",
      "('university', 'press', 'isbn') - 131\n",
      "\n",
      "('a', 'number', 'of') - 129\n",
      "\n",
      "('a', 'set', 'of') - 125\n",
      "\n",
      "('based', 'on', 'the') - 118\n",
      "\n",
      "\n",
      "Top-5 words of 'Natural language processing'\n",
      "\n",
      "language - 42\n",
      "\n",
      "processing - 26\n",
      "\n",
      "text - 25\n",
      "\n",
      "natural - 21\n",
      "\n",
      "speech - 20\n",
      "\n",
      "\n",
      "Top-5 3grams of 'Natural language processing'\n",
      "\n",
      "('natural', 'language', 'processing') - 8\n",
      "\n",
      "('a', 'chunk', 'of') - 6\n",
      "\n",
      "('chunk', 'of', 'text') - 6\n",
      "\n",
      "('of', 'natural', 'language') - 5\n",
      "\n",
      "('systems', 'based', 'on') - 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment().show_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
